{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "from string import ascii_lowercase\n",
    "from time import time\n",
    "\n",
    "from wordfreq import get_frequency_dict\n",
    "\n",
    "from distances import levenshtein\n",
    "from phonetic_algs import soundex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a dictionary\n",
    "The frequency dictionary contains words along with their relative frequencies provided by `wordfreq` that are also present in `abc_en` list (concatenated word lists of **A**merican, **B**ritish and **C**anadian English). This combination keeps regional spellings, e.g., *splendor* and *splendour*, *analyze* and *analyse*, and, at the same time, gets rid of words that appear too seldom (have extremely low frequency, like *veery*, + some probable typos: *leik*, *corect* and so on). When dealing solely with non-word errors, we can't afford the presense of such rare words in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freqd = get_frequency_dict(lang='en', wordlist='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# of words in wordfreq's English list: **302,001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"abc_en.txt\"\n",
    "\n",
    "with open(dict_path) as f:\n",
    "    abc = tuple(line.strip().lower() for line in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# of words in abc_en file: **104,224**\n",
    "\n",
    "**Note!**\n",
    "The file `abc_en.txt` includes words of [American](https://packages.ubuntu.com/focal/wamerican), [British](https://packages.ubuntu.com/focal/wbritish) and [Canadian English](https://packages.ubuntu.com/focal/wcanadian) taken from the corresponding packages for Ubuntu Linux distibution. You can fetch these three word lists and easily restore the common list as `sorted(set(american + british + canadian))`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_d = {k: v for k, v in freqd.items() if k in abc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note!** It makes sense to save `freq_d` (using `json.dump` or other methods).\n",
    "\n",
    "Total \\# of words in the combined dictionary: **75,733**. The keys are lowercased and include different word forms, e.g., plurals and possessives (*cat - cats - cat's*), verbal forms and contractions (*swim - swam - swum - swimming, i'm - she'll - they'd*).\n",
    "\n",
    "So far, so good. Now, we are going to index these words using the Soundex algorithm.\n",
    "\n",
    "The results will be stored in another dictionary of the following structure:\n",
    "\n",
    "`{soundex_index_0: ['tea', 'too', 'two'], soundex_index_1: [], ..., soundex_index_n: [...]}`.\n",
    "\n",
    "This will save us a bit of time later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sndx_d = defaultdict(list)\n",
    "length = 8\n",
    "for word in freq_d:\n",
    "    sndx_d[soundex(word, length=length)].append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note! Why do we need pronunciation models?\n",
    "\n",
    "The fact that pronunciation of words tends to affect their (mis)spellings draws attention to letter-to-sound or grapheme-to-morpheme conversions. In linguistic terms, the extent to which a written language deviates from one-to-one letter-phoneme correspondence is described by the concept of *orthographic depth*. In languages with *shallow (transparent) orthographies*, like Finnish, Italian, or Ukrainian, it's easy to pronounce a word based on its spelling. In contrast, *deep (opaque) orthographies* are difficult to read letter-by-letter. French, English, or Swedish words may sound counterintuitively to their spellings due to complex orthographies of these languages. In spelling correction, edit distances, e.g., Levenshtein, work well for languages with small (shallow) orthographic depth. Yet, they don't bode well for languages with deep orthographies, which also require pronunciation being considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_ind = len(max(sndx_d.values(), key=len))\n",
    "# avg_ind = sum(len(v) for v in sndx_d.values()) / len(sndx_d)\n",
    "# print(len(sndx_d), max_ind, avg_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# of unique Soundex indexes amounts to **23,143**. The length of 8 symbols is used to reduce the number of words per index. Indexes can be refined further, but as the length grows the biggest number of words in a list levels off (334 per index with default value 4, 191 words -- with length 5+). The average amount of words per index decreases, too. At length 8, it's approximately 3 compared to 16 at the very beginning.\n",
    "There's a tradeoff between speed and accuracy. Tweak the `length` parameter to see it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for training and evaluation\n",
    "We'll use a [list of common misspellings from Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines). See [alternative options](http://www.dcs.bbk.ac.uk/~ROGER/corpora.html). \n",
    "\n",
    "First, we transform the original `misspelling->correct_word1[, correct_word_2, ...]` to one-to-one mapping `(misspelling, correct_word)` leaving only the pairs with the correct word in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_common_misspellings.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "            \n",
    "error_word = [(error, word) for (error, words) in (line.lower().split('->') for line in lines)\n",
    "                            for word in words.split(', ') if word in freq_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(error_word))    # 4299 pairs\n",
    "# print(error_word[17])     # ('abreviation', 'abbreviation')\n",
    "# # 7 misspellings: ['hten', 'teh', 'tghe', 'ther', 'thge', 'tje', 'tjhe']\n",
    "# print([error for (error, word) in error_word if word == 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the set in half for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(error_word)  # the original list was sorted alphabetically\n",
    "development_set = error_word[:2150]\n",
    "final_test_set = error_word[2150:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection mechanism\n",
    "Given a word `w`, we are trying to find such a correction `c` that, out of all possible candidates, maximizes the probability `P(c|w)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word, model=spell_1):\n",
    "    return max(candidates(word, model=model), key=lambda c: freq_d.get(c, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our datasets, all correct words are known. However, if the above function is called on an unknown word, the probability is set to 0.0. Other word frequencies are already stored as probabilities (floating-point numbers between 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error model\n",
    "When generating candidates, we use a massive simplification:\n",
    "\n",
    "- If a word is in the dictionary, as a correction of itself, it has the highest probability, which is not necessarily so. It sweeps away the second-largest class of spelling errors -- *RWEs*, or *real-word errors*.\n",
    "- The less is the edit distance to any valid word, the more probable this correction is\\* (there are different models below).\n",
    "- The last and least probable, the word itself, when not listed in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(word, model):\n",
    "    return known([word]) or model(word) or [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    return set(w for w in words if w in freq_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next logical step would be to introduce a noisy channel model, which can tell us how likely each particular misspelling is given the correct word.\n",
    "\n",
    "*Use weighted edit distance: for any given pair of letters, how likely are different single edits to happen?*\n",
    "\n",
    "What is really meant by maximizing `P(c|w)` is the following probability product:\n",
    "\n",
    "`P(c|w) = (P(w|c) * P(c)) / P(w)` (by Bayes' rule)\n",
    "\n",
    "As `P(w)` is constant, the formula is often shortened to\n",
    "`P(w|c) * P(c)`,\n",
    "\n",
    "where `P(w|c)` is a channel model / error model / likelihood,\n",
    "`P(c)` is a language model / prior.\n",
    "\n",
    "The noisy channel model can also be counted as the sum of logs of probabilities: `P(c|w) = log(P(w|c)) + b * log(P(c))`, with coefficient `b` to weigh the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code prototype\n",
    "```\n",
    "def noisy_channel(w, b=1.0):\n",
    "    scores = {}\n",
    "    for c, e in candidates(w):\n",
    "        channel = editprob(e)\n",
    "        prior = freq_d.get(w, 0.0)\n",
    "        score[c] = log(channel) + b*log(prior)\n",
    "    return max(scores, key=scores.get)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate generation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive approach of computing the Levenshtein distance between a given word and every dictionary word is quite expensive. Instead, the approach suggested by Peter Norvig in [How to Write a Spelling Corrector](https://norvig.com/spell-correct.html) is used to find similarly spelled words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in ascii_lowercase]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in ascii_lowercase]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how we can use this to generate different lists of candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_1(word):\n",
    "    \"\"\"Returns known words with the smallest edit distance of 1, 2.\"\"\"\n",
    "    return known(edits1(word)) or known(edits2(word))\n",
    "\n",
    "def spell_2(word):\n",
    "    \"\"\"Returns known words with edit distance of 1 and 2.\"\"\"\n",
    "    return known(edits1(word)) | known(edits2(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find words with close pronunciation, we compute the Soundex index for a given word and compare it (yes, using actual Levenshtein distance) with Soundex indices from the dictionary we defined earlier.\n",
    "\n",
    "Notice that the distance of 0 doesn't correspond to the same word, rather, it matches all words with the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sound_0(word):\n",
    "    \"\"\"Returns known words with the same Soundex indexes.\"\"\"\n",
    "    ind = soundex(word, length=length)\n",
    "    return sndx_d.get(ind, [])\n",
    "\n",
    "def sound_1(word):\n",
    "    \"\"\"Returns known words with the smallest edit distance of 0, 1 (Soundex indexes).\"\"\"\n",
    "    ind = soundex(word, length=length)\n",
    "    d0 = sndx_d.get(ind, [])\n",
    "    d1 = []\n",
    "    for i in sndx_d:\n",
    "        d = levenshtein(ind, i)\n",
    "        if d == 1:\n",
    "            d1.extend(sndx_d[i])\n",
    "    return d0 or d1\n",
    "\n",
    "def sound_2(word):\n",
    "    \"\"\"Returns known words with edit distance of 0 and 1 (Soundex indexes).\"\"\"\n",
    "    ind = soundex(word, length=length)\n",
    "    d01 = sndx_d.get(ind, [])\n",
    "    for i in sndx_d:\n",
    "        d = levenshtein(ind, i)\n",
    "        if d == 1:\n",
    "            d01.extend(sndx_d[i])\n",
    "    return d01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine spelling and pronunciation matching and experiment with this, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_sound_1(word):\n",
    "    \"\"\"Returns known words within edit distance 1 or with the same Soundex index.\"\"\"\n",
    "    ind = soundex(word, length=length)\n",
    "    return known(edits1(word)) or sndx_d.get(ind, [])\n",
    "\n",
    "def spell_sound_2(word):\n",
    "    \"\"\"Returns known words within edit distance 1 and with the same Soundex index.\"\"\"\n",
    "    ind = soundex(word, length=length)\n",
    "    return known(edits1(word)) | set(sndx_d.get(ind, []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the models we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (spell_1, spell_2, sound_0, sound_1, sound_2, spell_sound_1, spell_sound_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelltest(tests, verbose=False):\n",
    "    \"Runs correct(wrong) on all (wrong, right) pairs; reports results.\"\n",
    "    for model in models:\n",
    "        start = time()\n",
    "        good = 0\n",
    "        n = len(tests)\n",
    "        for wrong, right in tests:\n",
    "            w = correct(wrong, model)\n",
    "            good += (w == right)\n",
    "            if w != right and verbose:\n",
    "                print(f'correction({wrong}) => {w} ({freq_d[w]}); expected {right} ({freq_d[right]})')\n",
    "        dt = time() - start\n",
    "        print(f'{model.__name__}: {good/n:.0%} of {n} correct at {n/dt:.0f} words per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the development set to select the best model and/or perform error analysis afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelltest(development_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results:\n",
    "\n",
    "```\n",
    "spell_1: 83% of 2150 correct at 61 words per second\n",
    "spell_2: 50% of 2150 correct at 8 words per second\n",
    "sound_0: 44% of 2150 correct at 88234 words per second\n",
    "sound_1: 44% of 2150 correct at 2 words per second (Soundex indexes of all test words were in sndx_d)\n",
    "sound_2: 44% of 2150 correct at 2 words per second\n",
    "spell_sound_1: 80% of 2150 correct at 6214 words per second\n",
    "spell_sound_2: 57% of 2150 correct at 5975 words per second\n",
    "```\n",
    "\n",
    "The numbers might differ slightly since we shuffled the original dataset.\n",
    "\n",
    "We can use for test `spell_1`, which returns known words with the smallest edit distance of 1, 2, and `spell_sound_1`, which uses the candidates within edit distance of 1 or words with the same Soundex index. Both models do well; one is better at accuracy, the other is 100 times faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis & Future work\n",
    "\n",
    "**Hyphens**\n",
    "\n",
    "Although `freq_d` contains contractions and possessives (both are written with an apostrophe `'`), the Unix dictionary lacks hyphenated words, e.g., phrasal adjectives like *off-the-shelf*, *all-time* or *eco-friendly*, proper nouns like *Anne-Marie* or *Jean-Paul*, etc. Expanding the dictionary with these and similar words might be of benefit.\n",
    "\n",
    "**Spaces**\n",
    "\n",
    "We might also consider inserting spaces in a word (it can be a mistyped phrase!): *aswell* is *as well*, *withe* is *with the*.\n",
    "\n",
    "For now, the datasets exclude unknown words. Without this restriction, we should deal with the above problems.\n",
    "\n",
    "**Noisy channel: error model & language model**\n",
    "\n",
    "As you try different candidate generation models, you can see that the performance of those producing more candidates is significantly worse. This happens because the `correct` function selects the best match based on word frequencies alone (our language model). To make things better, we can try to compute probability for every particular edit (channel model) and introduce a coefficient for prior probability / look at the context, e.g., bigrams, to catch context-sensitive mistakes (language model).\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (spell_1, spell_sound_1)\n",
    "spelltest(final_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "```\n",
    "spell_1: 84% of 2149 correct at 55 words per second\n",
    "spell_sound_1: 80% of 2149 correct at 6217 words per second\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_0(word):\n",
    "    return known(edits1(word))\n",
    "\n",
    "models = (spell_0,)\n",
    "spelltest(development_set)\n",
    "spelltest(final_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the candidates in `spell_sound_1` come solely from spellings within edit distance 1 (the Soundex part is not used). As shown below, that's not true (compare to accuracy = 80%).\n",
    "\n",
    "```\n",
    "spell_0: 76% of 2150 correct at 5771 words per second\n",
    "spell_0: 75% of 2149 correct at 6382 words per second\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example = \"I'm graat ad everyfing but speling\".lower().split()\n",
    "\n",
    "# print(*[correct(word, model=spell_1) for word in example])\n",
    "# print(*[correct(word, model=spell_sound_1) for word in example])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
